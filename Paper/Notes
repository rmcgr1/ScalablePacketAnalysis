Research Paper Notes

reached: x x
didn't reach: x x x x x x 

Large Scale Network Analysis using Distributed Minimal Hosts... 


Abstract
What’s the problem?
Why should the reader care?
What have you accomplished?
What are the implications? 

Using network packet analyzers on large data sets is problematic due significant resource requirements...

Analyzing network traffic captures is necessary for network defense, intrusion and management. Analysis of large network captures requires proporitional memory and processing capacity. A popular network analyzer, wireshark can require --- memory to model the protocol state.  Deep packet inspection. The memory requirA single hosts may not be feasible to preform deep packet inspection within a multi-gigabyte capture file   This paper presents a technique to divide and distribute large packet captures to remote hosts which process and return analytic queries.  The system is designed to run with minimal installation, such as laptops ttransported to a client site, and the system has shown a speedup of .... This allows scaleable network analysis techniquces in a variety of situatiosns such aas incident response in at a short notice. 

####################

Introduction
Longer treatment of points in abstract
What previous work does paper build on?
emphasize the positive, more than shortcomings
Pointers to rest of paper 


When the invitable network intrusion occurs, it is critical to know exactly where, when, and what has been compromised. Packet analyzers, such as the open source wireshark, enable analysts to examine individual packets by understanding the structure of various network protocols and use a filtering syntax preform queries.  These queries, such as listing websites visted, or -- is called deep packet inspection is needed/useful....  However, with the increased preformance of networks, the sheer volume of normal traffic obscures a small malicitous event.  Searching mult-gigabyte arge datasets can overwhelm packet analyzers...

Multiple approaches to mine network data for security threats. have been develeoped. One is to create a summary of the metadata of the network traffic, such as listing source and destination IP addresses. (mention netflow here?)  This reduces the volume of data, but reqires prior knowledge on what is necessary to capture.  An intrusion detection system, such as Snort, scans traffic for pre-defined signatures and issues an alert. The open source Bro network security monitoring system records network state, scans network traffic anonlyms, and is designed to scale for a large network.  Other efforts have translated network data to put network data into a database system to query against. pcap2sql, These approaches can protect a network against known threats, but these lack the flexibility that an analysts would have with a packet analysizer with a small capture of data.  

Existing efforts have attempted to address this need to analyze large network captures. Network traffic has limited dependcies outside of an established session and can be split into parallel workloads for hosts to analyze individually.  Techniques from (lee and yeonhee) and (RIPE-CC) both use an Hadoop-based approach to distribute work among a number of hosts.  (lee and yeonhee)'s system injests netflow data and provides statistics while (RIPE-CC)'s system preforms a distributed DNS based analytic and has support for preforming SQL-like queries against the PCAP.  Both systems require the overhead and complexity of a Hadoop system, translation of the network data to an optimized format, and are tailored for a specific analytic uses.  Packetpig works on full network captures and utilizes Hadoop to enable Apache Pig queries against the traffic.  This enables access to specific chunks of network data 

This system presented in this paper enables the same tools used for small-scale packet analysis to be utilzed for parallel computation. The goal is to enable repeated network queries against a dataset with a lightweight design. The system is responsible for distributing a portion of the network data to each host, issuing commands, and collecting the results. Each host is required to have the analytic tool installed (tshark, tcpdump, and ngrep are supported), rsync, and ssh access.  All commands are issued over ssh and data transfers use rsync over ssh to compresses data and only transfers files that have changed.  These design principals enable a collection of hetrogenous hosts to rapidly divide a large network capture and run queries issued by a central host.  The central host optimizes for differences in computational node capabilties by redistributing the amount of data each host is responsible for based on a performence metric.  This improves repeated queries to the same, or a growing dataset. 

The system execues queries that are a tshark, tcpdump, or ngrep command.  These tools have a robust filtering syntax that is familiar to many in the network security community.  Buliding upon widely used open-source tools enables reuse of optimized packet processing libaries and support for hundreds of protocols.  Similar efforts that expose a SQL-like interface to query data are forced to translate, custom filter....lose query sytanx.  To match the hundreds of protocols decoded by wireshark, a custom sql-like implementation would have to identify each type of packet that may be of interest.

The system is designed to be a variety of use cases. One approach would be to develop the query on a subset of the data on a single machine.  Once this produces an expected result, this system can be used to quick scale this analytic to run in parallel on multiple hosts.  This system could be used to efficently filter traffic to generate output as for the next step of the analytic. A specific use case is to contruct a system of laptops that can be transported to a client site to be close to the captured network data.  Another use case would be to use a commerical cloud provider to add hosts as the amount of network data to search increases.


(repeted queries same data, increasing data, generate output for other analyitcs)

or evading the IDS, deep packet inspection is needed.

These approaches are effective for well known or structured network analytics.  When deep analytics are needed for identifying new threats, zero-day attacks,     Even if it is known exactly what to look for inside of a packet capture, sorting through the volume. ...   Packet analyzers interpret interpret show    

weakness of signature based IDS, you have to know what you're looking for

This system reduces analyzing large network datasets to a "pleasingly parallel" problem that can be distributed to any number of distributed hosts. The performance of this system, demonstrates a speedup of ///.  With repeated queries, the system adapts to a hetrogenous collection of distributed by transfering additional work to high-preforming hosts. 

This approach enables large-scale deep packet inspection using the right tool for the job.


Thoughts: work requires variety of tools. Enables analysts to use their tools for small-scale analytics to large scale.  Limitations of SQL/RDBMS, can't preform the deep analytics, parsing network traffic into intelligable SQL tables resutls in wasted computation unless previously know exactly what will be needed.  Utilizing own tool approach allows efecient parsing of headers, keyword, and binary signature searches.


Wireshark/Tshark

Ngrep

Netflow data

IDS 

Bro

The ablity to do effiencely mine large datasets of full network captures may lead to a differnt approach in network defense.  While an IDS or NSM  may indicate the presence of an adversary in the network, the ability the mine the full network capture of the adversary will reveal the tatics, the tools, and specpics of the damage done to the network.  With decreasing storage costs, an organization could augment exists security practices of storeing logs with a window/circular buffer of full network capture.

Hadoop based systems

SQL

#####################


Body
Highly variable, choose well
Logical progression with later sections
building on what has come before
Anticipate readers’ questions and give them
answers (or tell them when you have none) 

Design

The main functions of the system are to distribute the packet capture data among the processing nodes and to execute a packet analysis command across the nodes. For the distribute function, the control node splits the network packet captures into chunks which are distributed equally among the processing nodes.  Tcpdump is used to split the capture into 25 MB chunks. The chunk size is import in relation to the processing capacity of the host since analysis tools since the memory requirements to can be a multiple of the files size on disk (cite wireshark mailing list). This chunk size worked well for the limited resourced VMs used for development.  One downside to a small chunk length is that there is a greater chance that a session is split among chunks which may be an issue for some analytic queries.  Before files are distributed out to the nodes, the central node queries each node to see what files have already been distributed.  (Query is threaded??) .  The response is used to determine which files do need to be distributed out to the nodes.  The distribution is made using the rsync program.  Rsync can run over SSH, and the same channel is used to distrubute the files.     

The user can repeately distribute a changing set of network and only the difference with what is already out on the processing nodes is transferred.  The command function executes a user specified command for the tshark, ngrep, or tcpdump programs on each processing node against the dstribtued data.

Load balancing algorithm

record times, nodes that take longer, caluclate difference in times and find how many addtional files this difference it represents for each processing node. IF this difference is over a threshold, say 20%, distribute these files  

The system is intended to be used with a large sumber of linux based processing nodes with minimal requirements.  The node the user interacts with is called the control node.  The only requirements for the processing nodes is to have the packet analysis tool installed and that the control node must be able to login into every node with passwordless ssh certificates.  


Evaluation


Useage

Performance


Related Work
Be generous in citing others
do not limit to work that directly influenced paper
Distinguish your work without bad-mouthing
the competition 

Related work

An similar effort, "A Hadoop-based Packet Trace Processing Tool"



Summary/Conclusion
Brief recap
•
What have you not done?
•
A central deficit is that this system is effectively a centralized algorithm... Design choice, central datastore.... The cetralized nature of the system may limit the ability for the system to scale. 
This effort has not addressed reslizance to task failures.  A failure of a node will cause incomplete output.  The network files can be re-distributed to functioning nodes, but the running computation will be lost. 

How might others take it further? 

An improvement could be a true decentralized algorithm where nodes could use a gossip-like algorithm to effectively distribute files and tasks.  

Hardware limitations - all results hampered due to running on limited number of VMs. Not definitively answered the question of if this approach would scale.  

Commerical cloud aspect - spin up aws nodes
modify for snort, bro.  clustered implemnations exists, but this concept could be extended to create a readymade image of network intrusion tools to be deployed as needed.