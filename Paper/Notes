Research Paper Notes

reached: x x
didn't reach: x x x x x x 

Large Scale Network Analysis using Distributed Minimal Hosts... 


Abstract
What’s the problem?
Why should the reader care?
What have you accomplished?
What are the implications? 

Using network packet analyzers on large data sets is problematic due significant resource requirements...

Analyzing network traffic captures is necessary for network defense, intrusion and management. Analysis of large network captures requires proporitional memory and processing capacity. A popular network analyzer, wireshark can require --- memory to model the protocol state.  Deep packet inspection. The memory requirA single hosts may not be feasible to preform deep packet inspection within a multi-gigabyte capture file   This paper presents a technique to divide and distribute large packet captures to remote hosts which process and return analytic queries.  The system is designed to run with minimal installation, such as laptops ttransported to a client site, and the system has shown a speedup of .... This allows scaleable network analysis techniquces in a variety of situatiosns such aas incident response in at a short notice. 



Introduction
Longer treatment of points in abstract
What previous work does paper build on?
emphasize the positive, more than shortcomings
Pointers to rest of paper 


When the invitable network intrusion occurs, it is critical to know exactly where, when, and what has been compromised. Packet analyzers, such as the open source wireshark, enable analysts to examine individual packets by understanding the structure of various network protocols and use a filtering syntax preform queries.  These queries, such as listing websites visted, or -- is called deep packet inspection is needed/useful....  However, with the increased preformance of networks, the sheer volume of normal traffic obscures a small malicitous event.  Searching mult-gigabyte arge datasets can overwhelm packet analyzers...

Multiple approaches to mine network data for security threats. have been develeoped. One is to create a summary of the metadata of the network traffic, such as listing source and destination IP addresses. (mention netflow here?)  This reduces the volume of data, but reqires prior knowledge on what is necessary to capture.  An intrusion detection system, such as Snort, scans traffic for pre-defined signatures and issues an alert. The open source Bro network security monitoring system records network state, scans network traffic anonlyms, and is designed to scale for a large network.  Other efforts have translated network data to put network data into a database system to query against. pcap2sql, These approaches can protect a network against known threats, but these lack the flexibility that an analysts would have with a packet analysizer with a small capture of data.  

, or evading the IDS, deep packet inspection is needed.



Existing efforts have attempted to address this need to analyze large network captures. Network traffic can be split into parrallel workloads for remote hosts to analyze individually.  Techniques from (lee and yeonhee) and (RIPE-CC) both use an Hadoop-based approach to distribute work among a number of hosts.  (lee and yeonhee)'s system injests netflow data and provides statistics while (RIPE-CC)'s system preforms a distributed DNS based analytic and has support for preforming SQL-like queries against the PCAP.  Both systems require the overhead and complexity of a Hadoop system, translation of the network data to an optimized format, and are tailored for a specific analytic uses.  Packetpig works on full network captures and utilizes Hadoop to enable Apache Pig queries against the traffic.  This enables access to specific chunks of network data 


This system presented in this paper enables the same tools and techniques used for small-scale packet analysis to be distributed across a number of remote hosts. The goal is to enable multiple command-line packet analysis tools in a lightweight design.   The system is responsible for distributing network data to each host, issuing commands, and collecting the results. The requirements for each host are the analytic tool installed, rsync, and ssh access.  All commands and data transfers are done securely over ssh. This system is intended to be used with hetrogenous hosts.  It optimzes for host differences in repeated queries against the same traffic capture by redistributing the amount of data for each host based on a perforamance metric.  The system is designed to be deployed in a adhoc-manner. For instance, this system can run on a series of laptops transported to a client site to be close to the captured network datas.  Another use case would be to use a commerical cloud provider to add nodes as the amount of network data to search increases.

(repeted queries same data, increasing data, generate output for other analyitcs)



These approaches are effective for well known or structured network analytics.  When deep analytics are needed for identifying new threats, zero-day attacks,     Even if it is known exactly what to look for inside of a packet capture, sorting through the volume. ...   Packet analyzers interpret interpret show    

weakness of signature based IDS, you have to know what you're looking for


This system reduces analyzing large network datasets to a "pleasingly parallel" problem that can be distributed to any number of distributed hosts. The performance of this system, demonstrates a speedup of ///.  With repeated queries, the system adapts to a hetrogenous collection of distributed by transfering additional work to high-preforming hosts. 

This approach enables large-scale deep packet inspection using the right tool for the job.


Thoughts: work requires variety of tools. Enables analysts to use their tools for small-scale analytics to large scale.  Limitations of SQL/RDBMS, can't preform the deep analytics, parsing network traffic into intelligable SQL tables resutls in wasted computation unless previously know exactly what will be needed.  Utilizing own tool approach allows efecient parsing of headers, keyword, and binary signature searches.


Wireshark/Tshark

Ngrep

Netflow data

IDS 

Bro

The ablity to do effiencely mine large datasets of full network captures may lead to a differnt approach in network defense.  While an IDS or NSM  may indicate the presence of an adversary in the network, the ability the mine the full network capture of the adversary will reveal the tatics, the tools, and specpics of the damage done to the network.  With decreasing storage costs, an organization could augment exists security practices of storeing logs with a window/circular buffer of full network capture.

Hadoop based systems

SQL




Body
Highly variable, choose well
Logical progression with later sections
building on what has come before
Anticipate readers’ questions and give them
answers (or tell them when you have none) 

Design

The main functions of the system are to distribute the packet capture data among the processing nodes and to execute a packet analysis command across the nodes. In the distribute function, the control node splits the network packet captures into chunks which are distributed equally among the processing nodes. The user can repeately distribute a changing set of network and only the difference with what is already out on the processing nodes is transferred.  The command function executes a user specified command for the tshark, ngrep, or tcpdump programs on each processing node against the dstribtued data.

Load balancing algorithm

record times, nodes that take longer, caluclate difference in times and find how many addtional files this difference it represents for each processing node. IF this difference is over a threshold, say 20%, distribute these files  

The system is intended to be used with a large sumber of linux based processing nodes with minimal requirements.  The node the user interacts with is called the control node.  The only requirements for the processing nodes is to have the packet analysis tool installed and that the control node must be able to login into every node with passwordless ssh certificates.  


Evaluation


Useage

Performance


Related Work
Be generous in citing others
do not limit to work that directly influenced paper
Distinguish your work without bad-mouthing
the competition 

Related work

An similar effort, "A Hadoop-based Packet Trace Processing Tool"



Summary/Conclusion
Brief recap
•
What have you not done?
•
A central deficit is that this system is effectively a centralized algorithm... Design choice, central datastore.... The cetralized nature of the system may limit the ability for the system to scale. 
This effort has not addressed reslizance to task failures.  A failure of a node will cause incomplete output.  The network files can be re-distributed to functioning nodes, but the running computation will be lost. 

How might others take it further? 

An improvement could be a true decentralized algorithm where nodes could use a gossip-like algorithm to effectively distribute files and tasks.  

Hardware limitations - all results hampered due to running on limited number of VMs. Not definitively answered the question of if this approach would scale.  

Commerical cloud aspect - spin up aws nodes
modify for snort, bro.  clustered implemnations exists, but this concept could be extended to create a readymade image of network intrusion tools to be deployed as needed.