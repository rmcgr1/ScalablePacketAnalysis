Research Paper Notes

reached: x x
didn't reach: x x x x x x 

Distributed Network Analysis using Minimal Hosts... 


Abstract
What’s the problem?
Why should the reader care?
What have you accomplished?
What are the implications? 

Using network packet analyzers on large data sets is problematic due significant resource requirements...

Analyzing network traffic captures is necessary for network defense, intrusion and management. As a network captures increases, analysis requires proporitional memory and processing capacity. A popular network analyzer, wireshark can require --- memory to model the protocol state.  Deep packet inspection.  This paper presents a technique to divide and distribute large packet captures to remote hosts which process and return analytic queries.  The system is designed to run with minimal installation, such as laptops ttransported to a client site, and the system has shown a speedup of .... This allows scaleable network analysis techniquces in a variety of situatiosns such aas incident response in at a short notice. 



Introduction
Longer treatment of points in abstract
What previous work does paper build on?
emphasize the positive, more than shortcomings
Pointers to rest of paper 


High-impact network instrusions into large corporations have increased the importance of securing netowrks and when the inevitable network intrusion occurs, knowing exactly where, when, and what has been compromised is critical. With increased preformance of networks, the volume of traffic obscures a small malicitous event. Multiple approaches have been develeoped. One is to create netflows of network traffic. Netflows are brief summaries of the metatadata of network events, such as source and destination IP addresses.  This reduces the volume of data, but signfigant loss /// compared to deep packet inspection. Netflow flogs can be preformance intesive to search through and techniques from lee, yeonhee and RIPE-CC both use an Hadoop-based approach to distribute netflow analysis among a number of hosts.  Another approach to distributed network analytics is run a distributed intrusion detection system (IDS) such as snort or bro.  These tools scan network traffic for known patterns, or signatures, and warn of malicious activity.  Multiple IDSes can be deployed and forward warnings to a central server.  ____ describe methods to sort .    

These approaches are effective for well known or structured network analytics.  When deep analytics are needed for identifying new threats, zero-day attacks,     Even if it is known exactly what to look for inside of a packet capture, sorting through the volume. ...   Packet analyzers interpret interpret show    

weakness of signature based IDS, you have to know what you're looking for


This system reduces analyzing large network datasets to a "pleasingly parallel" problem that can be distributed to any number of distributed hosts. The performance of this system, demonstrates a speedup of ///.  With repeated queries, the system adapts to a hetrogenous collection of distributed by transfering additional work to high-preforming hosts. 

This approach enables large-scale deep packet inspection using the right tool for the job.   





Thoughts: work requires variety of tools. Enables analysts to use their tools for small-scale analytics to large scale.  Limitations of SQL/RDBMS, can't preform the deep analytics, parsing network traffic into intelligable SQL tables resutls in wasted computation unless previously know exactly what will be needed.  Utilizing own tool approach allows efecient parsing of headers, keyword, and binary signature searches.


Wireshark/Tshark

Ngrep

Netflow data

IDS 

Bro

The ablity to do effiencely mine large datasets of full network captures may lead to a differnt approach in network defense.  While an IDS or NSM  may indicate the presence of an adversary in the network, the ability the mine the full network capture of the adversary will reveal the tatics, the tools, and specpics of the damage done to the network.  With decreasing storage costs, an organization could augment exists security practices of storeing logs with a window/circular buffer of full network capture.

Hadoop based systems

SQL




Body
Highly variable, choose well
Logical progression with later sections
building on what has come before
Anticipate readers’ questions and give them
answers (or tell them when you have none) 

Overview

The main functions of the system are to distribute the packet capture data among the processing nodes and to execute a packet analysis command across the nodes. In the distribute function, the control node splits the network packet captures into chunks which are distributed equally among the processing nodes. The user can repeately distribute a changing set of network and only the difference with what is already out on the processing nodes is transferred.  The command function executes a user specified command for the tshark, ngrep, or tcpdump programs on each processing node against the dstribtued data.

Load balancing algorithm

record times, nodes that take longer, caluclate difference in times and find how many addtional files this difference it represents for each processing node. IF this difference is over a threshold, say 20%, distribute these files  

The system is intended to be used with a large sumber of linux based processing nodes with minimal requirements.  The node the user interacts with is called the control node.  The only requirements for the processing nodes is to have the packet analysis tool installed and that the control node must be able to login into every node with passwordless ssh certificates.  


Useage

Performance


Related Work
Be generous in citing others
do not limit to work that directly influenced paper
Distinguish your work without bad-mouthing
the competition 

Related work

An similar effort, "A Hadoop-based Packet Trace Processing Tool"



Summary/Conclusion
Brief recap
•
What have you not done?
•
A central deficit is that this system is effectively a centralized algorithm... Design choice, central datastore.... The cetralized nature of the system may limit the ability for the system to scale. 
This effort has not addressed reslizance to task failures.  A failure of a node will cause incomplete output.  The network files can be re-distributed to functioning nodes, but the running computation will be lost. 

How might others take it further? 

An improvement could be a true decentralized algorithm where nodes could use a gossip-like algorithm to effectively distribute files and tasks.  

Hardware limitations - all results hampered due to running on limited number of VMs. Not definitively answered the question of if this approach would scale.  

Commerical cloud aspect - spin up aws nodes
modify for snort, bro.  clustered implemnations exists, but this concept could be extended to create a readymade image of network intrusion tools to be deployed as needed.