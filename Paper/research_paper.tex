\documentclass{article}
\usepackage[margin={4cm,4cm}]{geometry}
\usepackage{multicol}

\begin{document}

\title{Large Scale Network Analytics Using Distributed Servers}
\author{Ryan McGrath}
\maketitle

\begin{abstract}

  Analysis of network traffic is necessary for network management and defense. However, analysis of large network packet captures is difficult due to proportional memory and processing requirements for the quantity of network data. A packet capture can have a memory requirement of ten times its size (cite). This makes it difficult for a single host to analyze multi-gigabyte capture files.  This paper presents a system to divide, distribute, and query large packet captures using a distributed system of lightweight servers. This enables network analysis techniques for small network captures to scale to gigabyte-sized captures.


\end{abstract}


\begin{multicols}{2}
  
\section*{Introduction} % \section* would create section without section
                       % number.


Packet analyzers, such as the open source Wireshark, enable analysts to examine individual packets.  An analyzer decodes the structure of network protocols encapsulated in the packet and enables queries against the data using a filtering syntax.  These queries can uncover misconfigured networks, misbehaving users, or even network intrusions. The overwhelming volume of normal traffic can make it difficult to discover limited, infrequent malicious events. This difficulty is compounded as the dataset scales. Packet analyzers have a high memory requirement because they reassemble packet sessions and model layers of network protocols.  Other packet analysis tools, such as tcpdump and ngrep, do not create dependencies between packets and filter selected packets in a single pass. While these tools have a lower resource requirement, they are time-consuming on large datasets. 

This paper presents a system to enable packet analyzers to analyze large datasets by using a distributed system. The system distributes a portion of the network data to each server, issues commands, and collects the results. A central server issues commands over SSH and transfers files using rsync to processing servers. This enables a collection of heterogeneous servers to rapidly divide a large network capture and run queries in parallel. The central server can adapt for differences in server resource capabilities and data by redistributing the chunks of data each server processes. This improves repeated queries against the same dataset. 

The syntax of the queries are tshark, tcpdump, or ngrep commands. These tools have a robust filtering syntax that is well-known. Building upon widely used open-source tools enables reuse of optimized packet processing libraries and support for hundreds of protocols. This avoids the complication of translating network data to fit a specific query syntax.  

The system is designed for a variety of use cases. One use case is to develop the query on a subset of the packet capture on a single machine and use the system to run the query against the full dataset.  Another use case is to filter a large dataset to produce output for a consuming analytic. The output can be sent using unix pipes to tools such as sort, uniq, or count. 

The system can be implemented using a commercial cloud provider to add additional servers until the desired performance is reached. Alternatively, laptops could be transported to a client site as a tool for network incident response.

\section*{Related Work}

Several approaches have been developed to analyze traffic and monitor networks. One approach, commonly called netflow, is to reduce the size of the captured network data by storing the metadata. This metadata, such as listing source and destination IP addresses, can be analyzed for anomalies.  This approach requires prior knowledge of what is necessary to capture, and discards most network content. Intrusion Detection Systems (IDS), such as Snort, take a different approach by scanning traffic for pre-defined signatures. The open source Bro network security monitoring system combines a netflow approach with IDS-like functionality.  Other efforts, such as pcap2sql, translate relevant network events into a database. These approaches can provide insight into a network, but are limited by the requirement to know exactly what to extract from network traffic ahead of an event. 

Other efforts have focused specifically on analyzing large network captures. Network traffic has limited dependencies and can be split into parallel workloads for servers to analyze individually.  Techniques from (cite lee and yeonhee) and (cite RIPE-CC) both use a Hadoop-based approach to distribute work among a number of servers.  (cites lee and yeonhee)'s system ingests netflow data and utilizes custom analytics for network statistics while (RIPE-CC) has developed a distributed DNS-based analytic and supports SQL-like queries against the captured data.  Both systems require the overhead and complexity of a Hadoop system, translation of the network data to an optimized format, and are tailored for specific analytic uses.  Packetpig is another effort that works on large network captures and utilizes Hadoop to run Apache Pig queries against captured traffic. 

\section*{Methods and Design}

The three primary components of the system are the distribute function, the command function, and the load balancing function. The distribute function splits the packet capture data among the processing servers. The command function executes the packet analysis query on each server.  All remote communications utilizes SSH public-private key pairs to provide authentication and a secure channel.  

In the distribute function,  the central node splits the network packet captures into chunks which are distributed equally among the processing nodes.  A capture is split into 25 megabyte (MB) chunks. The chunk size is important because the memory requirment can be multiple of the files size on disk. This chunk size was selected because it preformed well with the limited resourced Virtual Machines that were used in development.  Before files are distributed, the central server queries each processing server for files have already been distributed. Then, only files which have changed are transfered.  This allows for efficient repeated queries of a dataset that continues to grow. The distribution is made using the rsync program running over SSH, and compressed.  Chunks are distributed in a round-robin method so if a single portion of the capture is more complex to analyze, there is a chance that it is distributed equally among the processing servers.      

In the command function, the central node executes a tshark, ngrep, or tcpdump command on each processing node against the distributed data. The central server uses the provide command to build a bash for-loop that runs on each processing server. An SSH session is established with each processing server concurrently using the python threading library.  Each processing node executes the command against their local allotment of the network data and returns the output to the central server. The data is collected in a thread-safe data structure and sent to standard output in order of completion.

In the load balancing function, the central server moves chunks between poorly performing servers and highly preforming servers to lower the disparity in completion times between nodes, and lower the overall task compltion time.  The central node records the task completion time for each node after a query.  If a processing node's completion time is above a user-specified percentage threshold of the average completion time of the system, the central node marks it as a slow server. If below the threshold, it is marked as a fast server.  The central node calculates a proportional number of randomly selected files to transfer from slow servers to fast servers. The number of files to transfer from the slow server is calculated by finding the average time expended per file.  Then the difference between the system average completion time and the server's completion time is divided by this time per file to find the number of files to tranfer. The randomly selected files from the slowest server are transfered to the fastest server.  The process is repeated with the second slowest and fastest servers, and so on, until there is not a pair of servers that performed outside of the threshold.  Transferring files in proportion to the difference from the average completion time attepts to get servers preforming closer to the average completion time quickly.  This accomplishes two goals, utilizing more of high resourced servers and leveling the distribution of sought-after network data throughout the cluster.  An example of sought-after data would be if a queries targets a specific network device, and traffic from this device only occurs in chunks of the network capture concentrated on a processing server.  Eventually, these sought-after chunks will be distributed among a greater number of nodes. 

\section*{Evaluation} 


The objective is to determine the degree of parallelism exhibited by parallelizing this operation and specifically to evaluate the effectivness of the load balancing technique. The system is configured to extract all DNS queries out of a 695 MB packet capture with different numbers of processing servers.  Each processing server is identical.  The dataset is split into 26 chunks, however the time to process each chunk is not equal.  Since the query only considers DNS traffic the amount of DNS traffic is the deciding factor as  other traffic can be discarded quickly.  Figure () shows the relevant statistics of the dataset.  The load balancing technique exploits this high disparity between chunks.  

\begin{tabular}{llr}
\hline
\multicolumn{2}{c}{Speedup} \\
\cline{1-2}
Size & 695 MB \\
Records & 27 \\
Sum & 17604 \\
Average & 653 \\ 
Standard Deviation & 526.83 \\
Median & 419 \\
Max & 2393 \\
Min & 80 \\
\hline
\end{tabular}




A limitation to evaluating this effort was a lack of hardware to test the system at scale. The test were conducted using Virtual Machines (VM) each allocated 512 MB of RAM and a single core processor.  All VMs are hosted on an 8 GB RAM, quad-core Apple iMac. Since resources need to be reserved for the host operating sytem, a limited number of VMs can run at once.  Tests with more VMs running can be affected by competition for processor time, disk access, background activity of the host, and the overhead of running the VMs.

Figure () shows the completion times for ten repeated queries with different numbers of processing servers.  The completion time decreases as the number of nodes increases, until more than 4 servers are used.  At this point, the overhead outpaces any gains from adding more nodes to divide the work.  This limit could be the result of of running more than 4 VMs affected the running time.  Since the host machine has a quad core processor, running more than four VMs causes contention for CPU resouces and increases the completion time.

Examining the performance of the four server cluster, figure () show the differnce in completion time when using the load balancing technique.  The green, broken line, shows the performance of four servers with randomly distributed chunks. The blue, solid line, shows the performance when the load balancing algorithm is applied after one query with the random distribution of chunks.  After the first query there is a decrease in completinon time.  The load balancing algorithm can make a data swap that adversaly effects the running time because the average cost per file is calculated rather than knowing the actual cost. However, once effective swaps are made that brings the running time within the specified 10 percent threshold, the algorithm stops transfers until the threshold is breached.  The performance of the load balancing algorithm can be measured by the standard deviation of the individual server completion times. Figure () shows the standard deviation in server completion times when using four servers and eight servers.  Without load balancing, there is a realtively constant standard deviation.  The effectivness of the load balancing is seen in the reduction of the standard deviation after the first query.  This reduction results in a lower overall task completion time.  


When adding capacity to a distributed system, the best performance is linear speedup.  This means that as the number of processing servers is doubled, the speedup would double. Speedup is defined as the execution time for a single server to complete the task divided by the execution time for the multiple server configuration.  For this system there is less than linear speedup, as the speedup from doubling the nodes from 1 to 2 results in a speedup of 1.77 instead of an ideal speedup of 2.  Figure () shows a table of the speedup and efficiency results.  Efficentcy is a metric that represents the utiliztion of each server by diving the speedup factor by the number of servers.  An efficentcy of 1 would indicate perfect speedup.  Figure () shows the efficentcy per node as the number of servers increases.  The blue, broken line shows the efficiency of the system with a random distribution of data to the servers, while the green, solid line represents the effeciency when using the load balancing algorithm.  The load balancing algorithm increases the efficentcy of the system over a random distribution, but both approaches decline past the limit of four servers.

These results provide encouragement that in spite of the testing hardware limitations, this technique would scale with dedicated hardware and larger datasets.


The speedup does continue to increase as the number of processing nodes increases. One related factor to the less than ideal speedup is that node completion times are unbalanced. If there was a low standard deviation, the work load would be equally spread between the servers resulting in an overall lower completion time.  %The load balancing algorithm decrease the standard deviation of completion times for the 8 server cluster by RESULT.  The speedup factor increased by RESULT.
These results provide encouragement that in spite of the testing hardware limitations, this technique would scale with dedicated hardware and larger d
atasets.




\begin{tabular}{llr}
\hline
\multicolumn{2}{c}{Speedup} \\
\cline{1-2}
Servers    & Speedup & Efficiency  \\
\hline
1     &  1  & 1   \\
2     &   1.77  & .88  \\
4     &   2.73  & .68 \\
4 (balance) & 2.97 & .74     \\
8 &  2.89   & .36  \\
8 (balance) & 2.93 & .36      \\
\hline
\end{tabular}


There are limitations of this system that were not addressed in this effort.  As the system scales, the central node can become overwhelmed with network traffic as it becomes a hotspot with all traffic leaving and returning to it. One extension could have nodes assisting with data transfer in a peer-to-peer arrangiment instead all transfers originating with the central node.  Another would be to handle node failures. One way would be to track the files that were on the failed node and to re-distributed them to a functioning node.  Another issue is the possibility for the system may be overwhelmed by a multitude of results. The result of a query may overflow the bounded buffer that is transferring the result back to the central node.   


\section*{Conclusion}

This system can efficiently analyze large network captures. This ability may enable different approaches in network management.  While an IDS may indicate the presence of an adversary in the network, in depth packet analysis is crucial to uncovering the details of damage done to the network.  An organization could augment existing network management practices by storing full network captures.  When unusal activity occours, the organization can look back to see exactly what happend on their network.  

\end{multicols}
\end{document}
